/*
 * Copyright (c) 2016-2018 "Neo4j Sweden, AB" [https://neo4j.com]
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * Attribution Notice under the terms of the Apache License 2.0
 *
 * This work was created by the collective efforts of the openCypher community.
 * Without limiting the terms of Section 6, any Derivative Work that is not
 * approved by the public consensus process of the openCypher Implementers Group
 * should not be described as “Cypher” (and Cypher® is a registered trademark of
 * Neo4j Inc.) or as "openCypher". Extensions by implementers or prototypes or
 * proposals for change that have been documented or implemented should only be
 * described as "implementation extensions to Cypher" or as "proposed changes to
 * Cypher that are not yet approved by the openCypher community".
 */
package org.opencypher.spark.integration

import java.io.File
import java.nio.charset.StandardCharsets
import java.nio.file.{Files, Paths}
import java.util.Calendar

import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.types.{StringType, StructField, TimestampType}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.opencypher.okapi.api.graph.Namespace
import org.opencypher.spark.api.{CAPSSession, GraphSources}
import org.opencypher.spark.integration.LDBCToGraphDDL._
import org.opencypher.spark.util.ConsoleApp

import scala.io.Source
import scala.util.Properties

/**
  * This demo reads data generated by the LDBC SNB data generator and performs the following steps:
  *
  * 1) Loads the raw CSV files into Hive tables
  * 2) Normalized tables according to the LDBC schema (i.e. place -> [City, Country, Continent]
  * 3) Generates a Graph DDL script based on LDBC naming conventions
  * 4) Creates a SQL PGDS based on the generated Graph DDL file
  * 5) Runs a Cypher query over the LDBC graph in Spark
  */
object LdbcIntegrationDemo extends ConsoleApp {

  implicit val resourceFolder: String = "/ldbc"
  val database = "LDBC"

  implicit val session: CAPSSession = CAPSSession.local(CATALOG_IMPLEMENTATION.key -> "hive")
  implicit val spark: SparkSession = session.sparkSession

  val csvFiles = new File(resource("csv/").getFile).list()

  spark.sql(s"DROP DATABASE IF EXISTS $database CASCADE")
  spark.sql(s"CREATE DATABASE $database")

  // Load LDBC data from CSV files into Hive tables
  csvFiles.foreach { csvFile =>
    spark.read
      .format("csv")
      .option("header", value = true)
      .option("inferSchema", value = true)
      .option("delimiter", "|")
      .load(resource(s"csv/$csvFile").getFile)
      // cast e.g. Timestamp to String
      .withCompatibleTypes
      .write
      .saveAsTable(s"$database.${csvFile.dropRight("_0_0.csv".length)}")
  }

  // Create views that normalize LDBC data where necessary
  val views = readFile(resource("sql/ldbc_views.sql").getFile).split(";")
  views.foreach(spark.sql)

  // generate GraphDDL file
  val graphDdlString = LDBCToGraphDDL.toGraphDDL("hive", database)
  println(graphDdlString)
  writeFile(resource("ddl/ldbc.ddl").getFile, graphDdlString)

  // create SQL PGDS
  val sqlGraphSource = GraphSources
    .sql(resource("ddl/ldbc.ddl").getFile)
    .withSqlDataSourceConfigs(resource("ddl/data-sources.json").getFile)

  session.registerSource(Namespace("sql"), sqlGraphSource)

  session.cypher(
    s"""
       |FROM GRAPH sql.LDBC
       |MATCH (n:Person)-[:islocatedin]->(c:City)
       |RETURN n.firstName, c.name
       |LIMIT 20
     """.stripMargin).show


  private def readFile(fileName: String) = Source.fromFile(fileName)
    .getLines()
    .mkString(Properties.lineSeparator)

  private def writeFile(fileName: String, content: String): Unit  =
    Files.write(Paths.get(fileName), content.getBytes(StandardCharsets.UTF_8))
}

object LDBCToGraphDDL {

  val excludeTables = Set(
    // normalized node tables
    "place",
    "organisation",
    // normalized relationship tables
    "person_islocatedin_place",
    "comment_islocatedin_place",
    "post_islocatedin_place",
    "organisation_islocatedin_place",
    "place_ispartof_place",
    "person_studyat_organisation",
    "person_workat_organisation",
    // list properties
    "person_email_emailaddress",
    "person_speaks_language")

  def toGraphDDL(datasource: String, database: String)(implicit spark: SparkSession): String = {
    // select all tables (including views)
    val tableNames = spark.sql(s"SHOW TABLES FROM $database").collect()
      .map(_.getString(1))
      .filterNot(excludeTables.contains)
      .toSet

    val nodeTables = tableNames.filterNot(_.contains("_"))
    val edgeTables = tableNames -- nodeTables

    val edgeLabels = edgeTables.map(_.split("_")(1))

    val labelDefinitions = toLabelDefinitions(database, nodeTables ++ edgeTables).toSeq.sorted
    val nodeTypeDefinitions = nodeTables.map(nodeTable => s"(${nodeTable.toNodeLabel})")
    val edgeTypeDefinitions = edgeLabels.map(edgeTable => s"[$edgeTable]")

    val edgeConstraints = edgeTables.map(_.split("_").toList).map {
      case startTable :: relType :: endTable :: Nil => s"(${startTable.toNodeLabel})-[$relType]->(${endTable.toNodeLabel})"
      case _ =>
    }

    val nodeLabelSets = nodeTables.map(nodeTable => s"(${nodeTable.toNodeLabel}) FROM $nodeTable")
    val edgeLabelSets = edgeTables.map(edgeTable => edgeTable -> edgeTable.split("_").toList).map {
      case (edgeTable, startTable :: relType :: endTable :: Nil) =>

        val startLabel = startTable.toNodeLabel
        val endLabel = endTable.toNodeLabel

        val startJoinExpr = if (startTable == endTable) {
          s"edge.$startLabel.id0 = node.id"
        } else {
          s"edge.$startLabel.id = node.id"
        }

        val endJoinExpr = if (startTable == endTable) {
          s"edge.$endLabel.id1 = node.id"
        } else {
          s"edge.$endLabel.id = node.id"
        }

        val startNodes = s"LABEL SET ($startLabel) FROM $startTable node JOIN ON $startJoinExpr"
        val endNodes = s"LABEL SET ($endLabel) FROM $endTable node JOIN ON $endJoinExpr"

        s"($relType) FROM $edgeTable edge START NODES $startNodes END NODES $endNodes"

      case _ =>
    }

    s"""
       |-- generated by ${getClass.getSimpleName} on ${Calendar.getInstance().getTime}
       |SET SCHEMA $datasource.$database
       |
       |${labelDefinitions.mkString(Properties.lineSeparator)}
       |
       |CREATE GRAPH SCHEMA ${database}_schema
       |    -- Node types
       |    ${nodeTypeDefinitions.mkString("," + Properties.lineSeparator + "\t")}
       |
       |    -- Edge types
       |    ${edgeTypeDefinitions.mkString("," + Properties.lineSeparator + "\t")}
       |
       |    -- Edge constraints
       |    ${edgeConstraints.mkString("," + Properties.lineSeparator + "\t")}
       |
       |CREATE GRAPH $database WITH GRAPH SCHEMA ${database}_schema
       |    NODE LABEL SETS (
       |        ${nodeLabelSets.mkString("," + Properties.lineSeparator + "\t\t")}
       |    )
       |
       |    RELATIONSHIP LABEL SETS (
       |        ${edgeLabelSets.mkString("," + Properties.lineSeparator + "\t\t")}
       |    )
       |
       """.stripMargin
  }

  private def toLabelDefinitions(database: String, tableNames: Set[String])(implicit spark: SparkSession): Set[String] =
    tableNames.map {
      tableName =>
        val tableInfos = spark.sql(s"DESCRIBE TABLE $database.$tableName").collect()

        val label = if (tableName.contains("_")) {
          tableName.split("_")(1)
        } else {
          tableName.toNodeLabel
        }
        val properties = tableInfos
          .filterNot(row => row.getString(0) == "id" || row.getString(0).contains(".id"))
          .map { row =>
            val propertyKey = row.getString(0)
            val propertyType = row.getString(1).toCypherType
            s"$propertyKey : $propertyType"
          }

        if (properties.nonEmpty) {
          s"CREATE LABEL ($label { ${properties.mkString(", ")} } )"
        } else {
          s"CREATE LABEL ($label)"
        }
    }

  implicit class StringHelpers(val s: String) extends AnyVal {

    def toNodeLabel: String = s.capitalize

    def toCypherType: String = s.toUpperCase match {
      case "STRING" => "STRING"
      case "BIGINT" => "INTEGER"
      case "INT" => "INTEGER"
      case "BOOLEAN" => "BOOLEAN"
      case "FLOAT" => "FLOAT"
      case "DOUBLE" => "FLOAT"
      // TODO: map correctly as soon as we support timestamp
      case "TIMESTAMP" => "STRING"
    }
  }

  implicit class DataFrameConversion(df: DataFrame) {

    def withCompatibleTypes: DataFrame = df.schema.fields.foldLeft(df) {
      case (currentDf, StructField(name, dataType, _, _)) if dataType == TimestampType =>
        currentDf
          .withColumn(s"${name}_tmp", currentDf.col(name).cast(StringType))
          .drop(name)
          .withColumnRenamed(s"${name}_tmp", name)

      case (currentDf, _) => currentDf
    }
  }
}
