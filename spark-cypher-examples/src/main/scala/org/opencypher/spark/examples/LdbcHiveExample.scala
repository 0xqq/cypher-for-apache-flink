package org.opencypher.spark.examples

import java.io.File

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.opencypher.okapi.api.graph.Namespace
import org.opencypher.spark.api.{CAPSSession, GraphSources}
import org.opencypher.spark.testing.utils.FileSystemUtils._
import org.opencypher.spark.util.LdbcUtil._
import org.opencypher.spark.util.{ConsoleApp, LdbcUtil}
/**
  * This demo reads data generated by the LDBC SNB data generator and performs the following steps:
  *
  * 1) Loads the raw CSV files into Hive tables
  * 2) Normalized tables according to the LDBC schema (i.e. place -> [City, Country, Continent]
  * 3) Generates a Graph DDL script based on LDBC naming conventions (if not already existing)
  * 4) Initializes a SQL PGDS based on the generated Graph DDL file
  * 5) Runs a Cypher query over the LDBC graph in Spark
  *
  * More detail about the LDBC SNB data generator are available under https://github.com/ldbc/ldbc_snb_datagen
  */
object LdbcHiveExample extends ConsoleApp {

  implicit val resourceFolder: String = "/ldbc"
  val database = "LDBC"

  implicit val session: CAPSSession = CAPSSession.local(CATALOG_IMPLEMENTATION.key -> "hive")
  implicit val spark: SparkSession = session.sparkSession

  val csvFiles = new File(resource("csv/").getFile).list()

  spark.sql(s"DROP DATABASE IF EXISTS $database CASCADE")
  spark.sql(s"CREATE DATABASE $database")

  // Load LDBC data from CSV files into Hive tables
  csvFiles.foreach { csvFile =>
    spark.read
      .format("csv")
      .option("header", value = true)
      .option("inferSchema", value = true)
      .option("delimiter", "|")
      .load(resource(s"csv/$csvFile").getFile)
      // cast e.g. Timestamp to String
      .withCompatibleTypes
      .write
      .saveAsTable(s"$database.${csvFile.dropRight("_0_0.csv.gz".length)}")
  }

  // Create views that normalize LDBC data where necessary
  val views = readFile(resource("sql/ldbc_views.sql").getFile).split(";")
  views.foreach(spark.sql)

  // generate GraphDdl file if it not exists
  if (!resoureExists("ddl/ldbc.ddl")) {
    val graphDdlString = LdbcUtil.toGraphDDL("hive", database)
    writeFile(resource("ddl").getFile + "/ldbc.ddl", graphDdlString)
  }

  // create SQL PGDS
  val sqlGraphSource = GraphSources
    .sql(resource("ddl/ldbc.ddl").getFile)
    .withSqlDataSourceConfigs(resource("ddl/data-sources.json").getFile)

  session.registerSource(Namespace("sql"), sqlGraphSource)

  session.cypher(
    s"""
       |FROM GRAPH sql.LDBC
       |MATCH (n:Person)-[:islocatedin]->(c:City)
       |RETURN n.firstName, c.name
       |ORDER BY n.firstName
       |LIMIT 20
     """.stripMargin).show
}
